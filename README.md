# Challenge Dados
Esse projeto √© desenvolvido atrav√©s do Challenge de Ci√™ncia de Dados da Alura. 
Este tem como objetivo fazer a classifica√ß√£o de pessoas inadimplentes da empresa Alura Cash, atrav√©s de um modelo de Machine Learning.
Ele ser√° desenvolvido atrav√©s da base de dados disponibilizada pela Alura, por√©m eu farei o tratamento dos dados e todo o resto do projeto sozinha.

> **Data de in√≠cio** : 29/08/22 

> **Linguagem utilizada** : Python 

> **Banco de dados** : MySQL

> **Visualiza√ß√£o de dados** : Power BI

# Semana 1
## Adicionando Banco de Dados e Tratando no Workbench 

Nessa primeira semana, comecei a organizar os passos que deveriam ser feitos com os dados. Notei algumas inconsist√™ncias: dados nulos, dados incertos e incoerentes, al√©m de visivelmente n√£o terem algum crit√©rio de organiza√ß√£o. 
Minha primeira contribui√ß√£o foi visualizar a situa√ß√£o, o que os dados queriam me dizer. Dessa forma, descobri que t√≠nhamos informa√ß√µes de in√∫meras pessoas, suas condi√ß√µes financeiras, quanto tempo trabalhavam na empresa e se elas j√° haviam sido inadimplentes no passado. Al√©m de ter a discri√ß√£o de empr√©stimos e suas finalidades.
A partir disso, √© poss√≠vel pensarmos a possibilidade de fazer uma predi√ß√£o, mais a frente nas semanas, das pessoas que est√£o sendo inadimplentes ou ter√£o a inten√ß√£o no futuro.
Uni as tabelas a partir do campo entre elas em comum (ID), utilizando a tabela IDS, usando INNER JOIN. Agora possuo todos os dados correspondente √†s pessoas. Exportei para CSV e a primeira semana do desafio est√° conclu√≠da! üöÄüî• 

# Semana 2
## Criando modelo de Classifica√ß√£o 
Nessa segunda semana tive alguns perrengues, por√©m consegui concluir esse modelo de classifica√ß√£o para o Alura Cash. Primeiramente eu importei toda as bibliotecas que iria precisar, juntamente com os dados exportados da primeira etapa do projeto. Depois de determinar quais features seriam necess√°rias para o modelo, exclui todas as outras que estariam atrapalhando o aprendizado da nossa m√°quina. Precisei fazer um tratamento de dados nulos e repetidos, necessitando excluir algumas colunas desnecess√°rias. Ap√≥s terminado esse processo, precisei fazer a identifica√ß√£o e tratamento de outliers, conjunto de dados que aprensentam uma quantidade exarcebante das demais, e, com isso, acabam atrapalhando nossa classifica√ß√£o. Fiz o encoding de vari√°veis categ√≥ricas, transformando elas em valores num√©ricos para passar pelo classificador e, finalmente, cheguei na parte a qual me deparei com problemas: o desbalanceamento de vari√°veis alvo. Como previsto, nossos dados da vari√°vel alvo (PESSOA_INADIMPLENTE) possuia uma diferen√ßa significativa entre 1 (pessoa inadimplente) e 0 (pessoa n√£o-inadimplente) a qual para 0 t√≠nhamos bem mais dados do que 1. Para solucionar esse problema, eu fiz uma extensa pesquisa de qual algoritmo seria adequado para o problema. Escolhi o Near Miss, um algoritmo do tipo Undersampling que "consiste manter todos os dados da classe com menor frequ√™ncia e diminuir a quantidade dos que est√£o na classe de maior frequ√™ncia, fazendo com que as observa√ß√µes no conjunto possuam dados com a vari√°vel alvo equilibrada.". A aplica√ß√£o do Near Miss foi bem simples, na verdade. Utilizei sua biblioteca python para selecionar uma nova sequ√™ncia de dados X e Y balanceado. Ap√≥s isso, apliquei no modelo de classifica√ß√£o LogisticRegression, ao qual acredito ser o mais adequado para o problema, e, por fim, fiz a valida√ß√£o do modelo atrav√©s do Score, Confusion_Matrix e ROC Curve.

#alurachallengedados1
